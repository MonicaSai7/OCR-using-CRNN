{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "CRNN_HDF5.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MonicaSai7/OCR-using-CRNN/blob/master/CRNN_HDF5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cLK4MC3CRo8",
        "colab_type": "text"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JcGeK8uCRpH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import string\n",
        "import cv2\n",
        "import h5py\n",
        "import fnmatch\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, BatchNormalization, Lambda, Bidirectional, LSTM, Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Afe_EIJCRpS",
        "colab_type": "text"
      },
      "source": [
        "# Data Acquisition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpXVr_dvCRpV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with h5py.File(\"/media/monica/New Volume/OCR/mjsynth_150000.hdf5\", 'r') as f:\n",
        "    training_img = f.get('train_img').value\n",
        "    orig_txt = f.get('train_labels').value\n",
        "    valid_img = f.get('valid_img').value\n",
        "    valid_orig_txt = f.get('valid_labels').value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSsEDVUnCRpm",
        "colab_type": "code",
        "colab": {},
        "outputId": "2a0701bc-502c-4502-f0fe-d8eb6043e59e"
      },
      "source": [
        "type(training_img[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jmopSF_CRpy",
        "colab_type": "code",
        "colab": {},
        "outputId": "6602fadb-27ba-44f0-de17-4674df929029"
      },
      "source": [
        "char_list = string.ascii_letters + string.digits\n",
        "print(char_list)\n",
        "print(len(char_list))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\n",
            "62\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C5rKU9oWCRp8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_to_labels(text):\n",
        "    code = []\n",
        "    for index, char in enumerate(text):\n",
        "        code.append(char_list.index(char))\n",
        "    return code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zizrXeLICRqL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "length_checker = np.vectorize(len) \n",
        "max_train_label_len = max(length_checker(orig_txt))\n",
        "max_test_label_len = max(length_checker(valid_orig_txt))\n",
        "max_label_len = max(max_train_label_len, max_test_label_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fj--UP4CRqU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_label_length = []\n",
        "train_input_length = []\n",
        "training_txt = []\n",
        "\n",
        "for i in range(len(orig_txt)):\n",
        "    train_label_length.append(len(orig_txt[i]))\n",
        "    train_input_length.append(31)\n",
        "    training_txt.append(encode_to_labels(orig_txt[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2DBGcVqCRqb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_label_length = []\n",
        "valid_input_length = []\n",
        "valid_txt = []\n",
        "\n",
        "for i in range(len(valid_orig_txt)):\n",
        "    valid_label_length.append(len(orig_txt[i]))\n",
        "    valid_input_length.append(31)\n",
        "    valid_txt.append(encode_to_labels(valid_orig_txt[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3nspS-pwCRql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_padded_txt = pad_sequences(training_txt, maxlen=max_label_len, padding='post', value = len(char_list))\n",
        "valid_padded_txt = pad_sequences(valid_txt, maxlen=max_label_len, padding='post', value = len(char_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-P7NtK6CRqw",
        "colab_type": "text"
      },
      "source": [
        "# Model Architecture\n",
        "\n",
        "### Model = CNN + RNN + CTC loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRtsVovWCRqz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input with shape of height=32 and width=128 \n",
        "inputs = Input(shape=(32,128,1))\n",
        " \n",
        "# convolution layer with kernel size (3,3)\n",
        "conv_1 = Conv2D(64, (3,3), activation = 'relu', padding='same')(inputs)\n",
        "# poolig layer with kernel size (2,2)\n",
        "pool_1 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_1)\n",
        " \n",
        "conv_2 = Conv2D(128, (3,3), activation = 'relu', padding='same')(pool_1)\n",
        "pool_2 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_2)\n",
        " \n",
        "conv_3 = Conv2D(256, (3,3), activation = 'relu', padding='same')(pool_2)\n",
        " \n",
        "conv_4 = Conv2D(256, (3,3), activation = 'relu', padding='same')(conv_3)\n",
        "# poolig layer with kernel size (2,1)\n",
        "pool_4 = MaxPool2D(pool_size=(2, 1))(conv_4)\n",
        " \n",
        "conv_5 = Conv2D(512, (3,3), activation = 'relu', padding='same')(pool_4)\n",
        "# Batch normalization layer\n",
        "batch_norm_5 = BatchNormalization()(conv_5)\n",
        " \n",
        "conv_6 = Conv2D(512, (3,3), activation = 'relu', padding='same')(batch_norm_5)\n",
        "batch_norm_6 = BatchNormalization()(conv_6)\n",
        "pool_6 = MaxPool2D(pool_size=(2, 1))(batch_norm_6)\n",
        " \n",
        "conv_7 = Conv2D(512, (2,2), activation = 'relu')(pool_6)\n",
        " \n",
        "squeezed = Lambda(lambda x: K.squeeze(x, 1))(conv_7)\n",
        " \n",
        "# bidirectional LSTM layers with units=128\n",
        "blstm_1 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(squeezed)\n",
        "blstm_2 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(blstm_1)\n",
        " \n",
        "outputs = Dense(len(char_list)+1, activation = 'softmax')(blstm_2)\n",
        " \n",
        "act_model = Model(inputs, outputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iy-ytzdbCRq6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = Input(name='the_labels', shape=[max_label_len], dtype='float32')\n",
        "input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
        "label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
        " \n",
        " \n",
        "def ctc_lambda_func(args):\n",
        "    y_pred, labels, input_length, label_length = args\n",
        " \n",
        "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
        " \n",
        " \n",
        "loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([outputs, labels, input_length, label_length])\n",
        "model = Model(inputs=[inputs, labels, input_length, label_length], outputs=loss_out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJUlAPYxCRrB",
        "colab_type": "code",
        "colab": {},
        "outputId": "eb8a2e91-837d-405e-c4ff-2ec47607fc5c"
      },
      "source": [
        "type(labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.framework.ops.Tensor"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dD-UcTnCCRrN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = 'adam')\n",
        " \n",
        "filepath=\"best_model.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRiiVBYPCRrT",
        "colab_type": "code",
        "colab": {},
        "outputId": "04028093-ad80-4e8a-daa6-c8df68ce91c0"
      },
      "source": [
        "training_img = np.array(training_img)\n",
        "train_input_length = np.array(train_input_length)\n",
        "train_label_length = np.array(train_label_length)\n",
        "\n",
        "valid_img = np.array(valid_img)\n",
        "valid_input_length = np.array(valid_input_length)\n",
        "valid_label_length = np.array(valid_label_length)\n",
        " \n",
        "batch_size = 256\n",
        "epochs = 10\n",
        "model.fit(x=[training_img, train_padded_txt, train_input_length, train_label_length], \n",
        "          y=np.zeros(len(training_img)), batch_size=batch_size, epochs = epochs, \n",
        "          validation_data = ([valid_img, valid_padded_txt, valid_input_length, \n",
        "                              valid_label_length], [np.zeros(len(valid_img))]), \n",
        "          verbose = 1, callbacks = callbacks_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 135000 samples, validate on 15000 samples\n",
            "Epoch 1/10\n",
            "   768/135000 [..............................] - ETA: 4:13:02 - loss: 61.5188"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EJqNN5akCRra",
        "colab_type": "code",
        "colab": {},
        "outputId": "608d09d9-7a2a-4765-9379-402b0a2a2735"
      },
      "source": [
        "# load the saved best model weights\n",
        "act_model.load_weights('best_model.hdf5')\n",
        "\n",
        "# predict outputs on validation images\n",
        "prediction = act_model.predict(valid_img)\n",
        " \n",
        "# use CTC decoder\n",
        "out = K.get_value(K.ctc_decode(prediction, input_length=np.ones(prediction.shape[0])*prediction.shape[1],\n",
        "                         greedy=True)[0][0])\n",
        " \n",
        "# see the results\n",
        "i = 0\n",
        "for x in out:\n",
        "    print(valid_orig_txt[i])\n",
        "    for p in x:  \n",
        "        if int(p) != -1:\n",
        "            print(char_list[int(p)], end = '')       \n",
        "    print('\\n')\n",
        "    i+=1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /home/monica/anaconda3/envs/deep-learning/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py:5783: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n",
            "Peens\n",
            "\n",
            "\n",
            "conceivably\n",
            "\n",
            "\n",
            "sanctity\n",
            "\n",
            "\n",
            "Disability\n",
            "\n",
            "\n",
            "lengthened\n",
            "\n",
            "\n",
            "Bushes\n",
            "\n",
            "\n",
            "digits\n",
            "\n",
            "\n",
            "PROBOSCISES\n",
            "\n",
            "\n",
            "oilmen\n",
            "\n",
            "\n",
            "Cellist\n",
            "\n",
            "\n",
            "Shin\n",
            "\n",
            "\n",
            "MOTIVELESS\n",
            "\n",
            "\n",
            "CATHARTIC\n",
            "\n",
            "\n",
            "Democracies\n",
            "\n",
            "\n",
            "calais\n",
            "\n",
            "\n",
            "MULTINATIONAL\n",
            "\n",
            "\n",
            "REPAVES\n",
            "\n",
            "\n",
            "MOUSIER\n",
            "\n",
            "\n",
            "repentance\n",
            "\n",
            "\n",
            "ESPADRILLE\n",
            "\n",
            "\n",
            "BOOTED\n",
            "\n",
            "\n",
            "moodiness\n",
            "\n",
            "\n",
            "downing\n",
            "\n",
            "\n",
            "corded\n",
            "\n",
            "\n",
            "yarmouth\n",
            "\n",
            "\n",
            "KANNADA\n",
            "\n",
            "\n",
            "unsightliness\n",
            "\n",
            "\n",
            "Sawyer\n",
            "\n",
            "\n",
            "impersonates\n",
            "\n",
            "\n",
            "Mwc\n",
            "\n",
            "\n",
            "Elegance\n",
            "\n",
            "\n",
            "Donates\n",
            "\n",
            "\n",
            "rucks\n",
            "\n",
            "\n",
            "ACCESSIBLE\n",
            "\n",
            "\n",
            "VALETS\n",
            "\n",
            "\n",
            "Gassiest\n",
            "\n",
            "\n",
            "Penetrates\n",
            "\n",
            "\n",
            "Sledgehammers\n",
            "\n",
            "\n",
            "incarnadined\n",
            "\n",
            "\n",
            "IMPERIOUSNESS\n",
            "\n",
            "\n",
            "LEO\n",
            "\n",
            "\n",
            "MUSTIEST\n",
            "\n",
            "\n",
            "DISGORGED\n",
            "\n",
            "\n",
            "Disprove\n",
            "\n",
            "\n",
            "Trieste\n",
            "\n",
            "\n",
            "hansom\n",
            "\n",
            "\n",
            "embracing\n",
            "\n",
            "\n",
            "Disrespectful\n",
            "\n",
            "\n",
            "UNSURE\n",
            "\n",
            "\n",
            "BARMAID\n",
            "\n",
            "\n",
            "Aced\n",
            "\n",
            "\n",
            "Decimalization\n",
            "\n",
            "\n",
            "endlessness\n",
            "\n",
            "\n",
            "Mousetraps\n",
            "\n",
            "\n",
            "ABJURATORY\n",
            "\n",
            "\n",
            "solemnities\n",
            "\n",
            "\n",
            "goodybagg\n",
            "\n",
            "\n",
            "UNSIGHTLIER\n",
            "\n",
            "\n",
            "Puss\n",
            "\n",
            "\n",
            "DADDIES\n",
            "\n",
            "\n",
            "grounders\n",
            "\n",
            "\n",
            "Decontaminates\n",
            "\n",
            "\n",
            "MACERATED\n",
            "\n",
            "\n",
            "Levels\n",
            "\n",
            "\n",
            "FRESNO\n",
            "\n",
            "\n",
            "DAMNABLE\n",
            "\n",
            "\n",
            "NAIR\n",
            "\n",
            "\n",
            "tigers\n",
            "\n",
            "\n",
            "therapeutics\n",
            "\n",
            "\n",
            "habitable\n",
            "\n",
            "\n",
            "ACTOR\n",
            "\n",
            "\n",
            "DELEGATING\n",
            "\n",
            "\n",
            "unbars\n",
            "\n",
            "\n",
            "rarefied\n",
            "\n",
            "\n",
            "Ballpoints\n",
            "\n",
            "\n",
            "histrionics\n",
            "\n",
            "\n",
            "idiosyncrasy\n",
            "\n",
            "\n",
            "wheelchairs\n",
            "\n",
            "\n",
            "ABSTRACTNESSES\n",
            "\n",
            "\n",
            "Uninfected\n",
            "\n",
            "\n",
            "buffoons\n",
            "\n",
            "\n",
            "Sufficing\n",
            "\n",
            "\n",
            "ROUSTABOUT\n",
            "\n",
            "\n",
            "guatemalan\n",
            "\n",
            "\n",
            "signorinas\n",
            "\n",
            "\n",
            "varmints\n",
            "\n",
            "\n",
            "IMPERSONATES\n",
            "\n",
            "\n",
            "Ming\n",
            "\n",
            "\n",
            "Incas\n",
            "\n",
            "\n",
            "Presbyters\n",
            "\n",
            "\n",
            "TIPSILY\n",
            "\n",
            "\n",
            "manky\n",
            "\n",
            "\n",
            "UNLOADING\n",
            "\n",
            "\n",
            "HUMPHREY\n",
            "\n",
            "\n",
            "blockheads\n",
            "\n",
            "\n",
            "DEPENDED\n",
            "\n",
            "\n",
            "Druggies\n",
            "\n",
            "\n",
            "Clattering\n",
            "\n",
            "\n",
            "REFUELING\n",
            "\n",
            "\n",
            "captivate\n",
            "\n",
            "\n",
            "SAIGON\n",
            "\n",
            "\n",
            "Sugarcoat\n",
            "\n",
            "\n",
            "profanely\n",
            "\n",
            "\n",
            "TORQUED\n",
            "\n",
            "\n",
            "PROTESTANTISMS\n",
            "\n",
            "\n",
            "mural\n",
            "\n",
            "\n",
            "WELDERS\n",
            "\n",
            "\n",
            "MILTING\n",
            "\n",
            "\n",
            "wayfaring\n",
            "\n",
            "\n",
            "Satisfactions\n",
            "\n",
            "\n",
            "Spirits\n",
            "\n",
            "\n",
            "ASTROLOGIST\n",
            "\n",
            "\n",
            "Schussing\n",
            "\n",
            "\n",
            "Gobs\n",
            "\n",
            "\n",
            "fedora\n",
            "\n",
            "\n",
            "PHASE\n",
            "\n",
            "\n",
            "Immoderately\n",
            "\n",
            "\n",
            "gu\n",
            "\n",
            "\n",
            "ARSENAL\n",
            "\n",
            "\n",
            "Bobtail\n",
            "\n",
            "\n",
            "protuberance\n",
            "\n",
            "\n",
            "trauma\n",
            "\n",
            "\n",
            "Xxxi\n",
            "\n",
            "\n",
            "Humidifying\n",
            "\n",
            "\n",
            "SUBLEASING\n",
            "\n",
            "\n",
            "Conking\n",
            "\n",
            "\n",
            "Epimethius\n",
            "\n",
            "\n",
            "norman\n",
            "\n",
            "\n",
            "SNOWS\n",
            "\n",
            "\n",
            "streptomycin\n",
            "\n",
            "\n",
            "Wallowing\n",
            "\n",
            "\n",
            "Oboe\n",
            "\n",
            "\n",
            "HINDQUARTERS\n",
            "\n",
            "\n",
            "benetton\n",
            "\n",
            "\n",
            "Nyetworks\n",
            "\n",
            "\n",
            "Rechristens\n",
            "\n",
            "\n",
            "BLOSSOMED\n",
            "\n",
            "\n",
            "ficus\n",
            "\n",
            "\n",
            "unicorns\n",
            "\n",
            "\n",
            "Sure\n",
            "\n",
            "\n",
            "Reconsigning\n",
            "\n",
            "\n",
            "Parsnip\n",
            "\n",
            "\n",
            "profs\n",
            "\n",
            "\n",
            "titties\n",
            "\n",
            "\n",
            "Spanks\n",
            "\n",
            "\n",
            "PREMISES\n",
            "\n",
            "\n",
            "Bulked\n",
            "\n",
            "\n",
            "FLOUR\n",
            "\n",
            "\n",
            "BOOKWORM\n",
            "\n",
            "\n",
            "gauchest\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNtExucaCRrk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrklMqTQCRrw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}