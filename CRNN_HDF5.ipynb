{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "CRNN_HDF5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MonicaSai7/OCR-using-CRNN/blob/master/CRNN_HDF5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNwEcP1SsR0M",
        "colab_type": "code",
        "outputId": "d71e4506-16a7-46c5-f70b-a73f82cd271f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHqwkWO1r1lw",
        "colab_type": "text"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IVd8BNlr1l3",
        "colab_type": "code",
        "outputId": "a2cd0fab-ab71-4889-a42c-95426203edb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 62
        }
      },
      "source": [
        "import os\n",
        "import string\n",
        "import cv2\n",
        "import h5py\n",
        "import fnmatch\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPool2D, BatchNormalization, Lambda, Bidirectional, LSTM, Dense"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PzdseLjr1mE",
        "colab_type": "text"
      },
      "source": [
        "# Data Acquisition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaUD8rP1r1mI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with h5py.File('gdrive/My Drive/mjsynth_150000.hdf5', 'r') as f:\n",
        "    for key in f.keys():\n",
        "        training_img = f.get('train_img').value\n",
        "        orig_txt = f.get('train_labels').value\n",
        "        valid_img = f.get('test_img').value\n",
        "        valid_orig_txt = f.get('test_labels').value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8snDzHCr1mS",
        "colab_type": "code",
        "outputId": "9211077b-b588-45c9-d8b8-e4744e703ee7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "training_img.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(135000, 32, 128, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "owZsY4Hxr1md",
        "colab_type": "code",
        "outputId": "84c0b227-30aa-408e-b3c9-847af0a5843e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "char_list = string.ascii_letters + string.digits\n",
        "print(char_list)\n",
        "print(len(char_list))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789\n",
            "62\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8nGXCFbr1mp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def encode_to_labels(text):\n",
        "    code = []\n",
        "    for index, char in enumerate(text):\n",
        "        code.append(char_list.index(char))\n",
        "    return code"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9HDLqGtr1m3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "length_checker = np.vectorize(len) \n",
        "max_train_label_len = max(length_checker(orig_txt))\n",
        "max_valid_label_len = max(length_checker(valid_orig_txt))\n",
        "max_label_len = max(max_train_label_len, max_valid_label_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ar21z20r1nJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_label_length = []\n",
        "train_input_length = []\n",
        "train_txt = []\n",
        "\n",
        "for i in range(len(orig_txt)):\n",
        "    train_label_length.append(len(orig_txt[i]))\n",
        "    train_input_length.append(31)\n",
        "    train_txt.append(encode_to_labels(orig_txt[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YukjsExIr1nR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_label_length = []\n",
        "valid_input_length = []\n",
        "valid_txt = []\n",
        "\n",
        "for i in range(len(valid_orig_txt)):\n",
        "    valid_label_length.append(len(orig_txt[i]))\n",
        "    valid_input_length.append(31)\n",
        "    valid_txt.append(encode_to_labels(valid_orig_txt[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2pMunBxwr1nb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_padded_txt = pad_sequences(train_txt, maxlen=max_label_len, padding='post', value = len(char_list))\n",
        "valid_padded_txt = pad_sequences(valid_txt, maxlen=max_label_len, padding='post', value = len(char_list))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhRhUSpIr1nt",
        "colab_type": "text"
      },
      "source": [
        "# Model Architecture\n",
        "\n",
        "### Model = CNN + RNN + CTC loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cxi6De8Ir1nw",
        "colab_type": "code",
        "outputId": "b446f3ea-261b-42d1-eab6-70c0517f0ffc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# input with shape of height=32 and width=128 \n",
        "inputs = Input(shape=(32,128,1))\n",
        " \n",
        "# convolution layer with kernel size (3,3)\n",
        "conv_1 = Conv2D(64, (3,3), activation = 'relu', padding='same')(inputs)\n",
        "# poolig layer with kernel size (2,2)\n",
        "pool_1 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_1)\n",
        " \n",
        "conv_2 = Conv2D(128, (3,3), activation = 'relu', padding='same')(pool_1)\n",
        "pool_2 = MaxPool2D(pool_size=(2, 2), strides=2)(conv_2)\n",
        " \n",
        "conv_3 = Conv2D(256, (3,3), activation = 'relu', padding='same')(pool_2)\n",
        " \n",
        "conv_4 = Conv2D(256, (3,3), activation = 'relu', padding='same')(conv_3)\n",
        "# poolig layer with kernel size (2,1)\n",
        "pool_4 = MaxPool2D(pool_size=(2, 1))(conv_4)\n",
        " \n",
        "conv_5 = Conv2D(512, (3,3), activation = 'relu', padding='same')(pool_4)\n",
        "# Batch normalization layer\n",
        "batch_norm_5 = BatchNormalization()(conv_5)\n",
        " \n",
        "conv_6 = Conv2D(512, (3,3), activation = 'relu', padding='same')(batch_norm_5)\n",
        "batch_norm_6 = BatchNormalization()(conv_6)\n",
        "pool_6 = MaxPool2D(pool_size=(2, 1))(batch_norm_6)\n",
        " \n",
        "conv_7 = Conv2D(512, (2,2), activation = 'relu')(pool_6)\n",
        " \n",
        "squeezed = Lambda(lambda x: K.squeeze(x, 1))(conv_7)\n",
        " \n",
        "# bidirectional LSTM layers with units=128\n",
        "blstm_1 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(squeezed)\n",
        "blstm_2 = Bidirectional(LSTM(128, return_sequences=True, dropout = 0.2))(blstm_1)\n",
        " \n",
        "outputs = Dense(len(char_list)+1, activation = 'softmax')(blstm_2)\n",
        " \n",
        "act_model = Model(inputs, outputs)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qydo3GsBr1n3",
        "colab_type": "code",
        "outputId": "218ca903-14cc-490d-9221-30ee2d25cd4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "labels = Input(name='the_labels', shape=[max_label_len], dtype='float32')\n",
        "input_length = Input(name='input_length', shape=[1], dtype='int64')\n",
        "label_length = Input(name='label_length', shape=[1], dtype='int64')\n",
        " \n",
        " \n",
        "def ctc_lambda_func(args):\n",
        "    y_pred, labels, input_length, label_length = args\n",
        " \n",
        "    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)\n",
        " \n",
        " \n",
        "loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name='ctc')([outputs, labels, input_length, label_length])\n",
        "model = Model(inputs=[inputs, labels, input_length, label_length], outputs=loss_out)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWcnkY-Or1oC",
        "colab_type": "code",
        "outputId": "1b0141e7-2581-4784-c58d-8e745eb6a145",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "type(labels)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensorflow.python.framework.ops.Tensor"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k53Ars_Br1oM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "model.compile(loss={'ctc': lambda y_true, y_pred: y_pred}, optimizer = 'adam')\n",
        " \n",
        "filepath=\"best_model.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='auto')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUiwququr1oU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training_img = np.array(training_img)\n",
        "train_input_length = np.array(train_input_length)\n",
        "train_label_length = np.array(train_label_length)\n",
        "\n",
        "valid_img = np.array(valid_img)\n",
        "valid_input_length = np.array(valid_input_length)\n",
        "valid_label_length = np.array(valid_label_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCWUS8H4GmPf",
        "colab_type": "code",
        "outputId": "9facf735-6e09-4417-c810-b289ddbe49ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 722
        }
      },
      "source": [
        "batch_size = 256\n",
        "epochs = 10\n",
        "model.fit(x=[training_img, train_padded_txt, train_input_length, train_label_length], \n",
        "          y=np.zeros(len(training_img)), batch_size=batch_size, epochs = epochs, \n",
        "          validation_data = ([valid_img, valid_padded_txt, valid_input_length, \n",
        "                              valid_label_length], [np.zeros(len(valid_img))]), \n",
        "          verbose = 1, callbacks = callbacks_list)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 135000 samples, validate on 15000 samples\n",
            "Epoch 1/10\n",
            "134912/135000 [============================>.] - ETA: 0s - loss: 30.2443\n",
            "Epoch 00001: val_loss improved from inf to 25.14164, saving model to best_model.hdf5\n",
            "135000/135000 [==============================] - 483s 4ms/sample - loss: 30.2437 - val_loss: 25.1416\n",
            "Epoch 2/10\n",
            "134912/135000 [============================>.] - ETA: 0s - loss: 27.8419\n",
            "Epoch 00002: val_loss improved from 25.14164 to 24.40798, saving model to best_model.hdf5\n",
            "135000/135000 [==============================] - 457s 3ms/sample - loss: 27.8407 - val_loss: 24.4080\n",
            "Epoch 3/10\n",
            "134912/135000 [============================>.] - ETA: 0s - loss: 21.1879\n",
            "Epoch 00003: val_loss improved from 24.40798 to 16.90544, saving model to best_model.hdf5\n",
            "135000/135000 [==============================] - 458s 3ms/sample - loss: 21.1842 - val_loss: 16.9054\n",
            "Epoch 4/10\n",
            "134912/135000 [============================>.] - ETA: 0s - loss: 15.1106\n",
            "Epoch 00004: val_loss improved from 16.90544 to 15.94511, saving model to best_model.hdf5\n",
            "135000/135000 [==============================] - 458s 3ms/sample - loss: 15.1080 - val_loss: 15.9451\n",
            "Epoch 5/10\n",
            "134912/135000 [============================>.] - ETA: 0s - loss: 14.0302\n",
            "Epoch 00005: val_loss improved from 15.94511 to 15.91277, saving model to best_model.hdf5\n",
            "135000/135000 [==============================] - 458s 3ms/sample - loss: 14.0300 - val_loss: 15.9128\n",
            "Epoch 6/10\n",
            "134912/135000 [============================>.] - ETA: 0s - loss: 13.5160\n",
            "Epoch 00006: val_loss did not improve from 15.91277\n",
            "135000/135000 [==============================] - 456s 3ms/sample - loss: 13.5150 - val_loss: 16.1503\n",
            "Epoch 7/10\n",
            "134912/135000 [============================>.] - ETA: 0s - loss: 13.1320\n",
            "Epoch 00007: val_loss did not improve from 15.91277\n",
            "135000/135000 [==============================] - 456s 3ms/sample - loss: 13.1330 - val_loss: 16.2503\n",
            "Epoch 8/10\n",
            "134912/135000 [============================>.] - ETA: 0s - loss: 12.8558\n",
            "Epoch 00008: val_loss did not improve from 15.91277\n",
            "135000/135000 [==============================] - 457s 3ms/sample - loss: 12.8572 - val_loss: 16.4451\n",
            "Epoch 9/10\n",
            "134912/135000 [============================>.] - ETA: 0s - loss: 12.6927\n",
            "Epoch 00009: val_loss did not improve from 15.91277\n",
            "135000/135000 [==============================] - 456s 3ms/sample - loss: 12.6935 - val_loss: 16.8873\n",
            "Epoch 10/10\n",
            "134912/135000 [============================>.] - ETA: 0s - loss: 12.4987\n",
            "Epoch 00010: val_loss did not improve from 15.91277\n",
            "135000/135000 [==============================] - 455s 3ms/sample - loss: 12.4997 - val_loss: 16.9383\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb47f0c7390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9VwPW_ur1op",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "891c3456-55de-4988-963f-5999bb0ea625"
      },
      "source": [
        "# load the saved best model weights\n",
        "act_model.load_weights('best_model.hdf5')\n",
        "\n",
        "# predict outputs on validation images\n",
        "prediction = act_model.predict(valid_img)\n",
        " \n",
        "# use CTC decoder\n",
        "out = K.get_value(K.ctc_decode(prediction, input_length=np.ones(prediction.shape[0])*prediction.shape[1],\n",
        "                         greedy=True)[0][0])\n",
        " "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/backend.py:5679: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Create a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2L2fR9X2r1o2",
        "colab_type": "code",
        "outputId": "17db9427-7a79-4c1c-8371-3f01194892a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# see the results\n",
        "i = 0\n",
        "count = 0\n",
        "for x in out:\n",
        "    #print(valid_orig_txt[i])\n",
        "    r = []\n",
        "    for p in x:  \n",
        "        if int(p) != -1:\n",
        "            #print(char_list[int(p)], end = '')   \n",
        "            r.append(char_list[int(p)])    \n",
        "    #print('\\n')\n",
        "    if valid_orig_txt[i] == ''.join(r):\n",
        "      count += 1\n",
        "    i+=1\n",
        "print(count,'/',i,'=',count/i)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6104 / 15000 = 0.4069333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq7GBsG_r1pB",
        "colab_type": "code",
        "outputId": "0706c605-beac-4fda-c50f-64277be9ad10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "6659/15000"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.44393333333333335"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9v-1XCC646n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}